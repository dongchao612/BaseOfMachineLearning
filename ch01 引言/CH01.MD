
# ch01

有一些鸢尾花的测量数据，属于setosa、versicolor 或virginica 三个品种之一。因为我们有已知品种的鸢尾花的测量数据，所以这是一个 **监督学习** 问题。
在这个问题中，我们要在多个选项中预测其中一个（鸢尾花的品种）。这是一个分类 **（classification）** 问题的示例。
可能的输出（鸢尾花的不同品种）叫作类别 **（class）** 。数据集中的每朵鸢尾花都属于三个类别之一，所以这是一个三分类问题。
单个数据点（一朵鸢尾花）的预期输出是这朵花的品种。对于一个数据点来说，它的品种叫作标签 **（label）**

## 初识数据
```text
key of iris_dataset ['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename']

target  ['setosa' 'versicolor' 'virginica']
feature ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']【花萼长度、花萼宽度、花瓣长度、花瓣宽度】

data.shape      (150, 4) 
target.shape    (150,)
```

## 训练数据与测试数据

一部分数据用于构建机器学习模型，叫作训练数据（training data）或训练集（training set）。其余的数据用来评估模型性能，叫作测试数据（test data）、测试集（test
set）或留出集（hold-out set）。

scikit-learn 中的train_test_split 函数可以打乱数据集并进行拆分。这个函数将75% 的行数据及对应标签作为训练集，剩下25% 的数据及其标签作为测试集。训练集与测试集的
分配比例可以是随意的，但使用25% 的数据作为测试集是很好的经验法则。

scikit-learn 中的数据通常用大写的X 表示，而标签用小写的y 表示。这是受到了数学标准公式f(x)=y 的启发，其中x 是函数的输入，y 是输出。我们用大写的X 是因为数据是一个二维数组（矩阵），用小写的y 是因为目标是一个一维数组（向量），这也是数学中的约定。

```python
X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'], iris_dataset['target'], random_state=0)
```

```python
print("X_train shape:{}".format(X_train.shape))  # (112, 4)
print("y_train shape:{}".format(y_train.shape))  # (112,)
print("X_test shape:{}".format(X_test.shape))  # (38, 4)
print("y_test shape:{}".format(y_test.shape))  # (38,)
```

在对数据进行拆分之前，train_test_split 函数利用伪随机数生成器将数据集打乱。如果我们只是将最后25% 的数据作为测试集，那么所有数据点的标签都是2，因为数据点是按标签排序的（参见之前iris['target'] 的输出）。测试集中只有三个类别之一，这无法告诉我们模型的泛化能力如何，所以我们将数据打乱，确保测试集中包含所有类别的数据。

为了确保多次运行同一函数能够得到相同的输出，我们利用random_state 参数指定了随机数生成器的种子。这样函数输出就是固定不变的，所以这行代码的输出始终相同。本书用到随机过程时，都会用这种方法指定random_state。

train_test_split 函数的输出为X_train、X_test、y_train 和y_test，它们都是NumPy数组。X_train 包含75% 的行数据，X_test 包含剩下的25%

## 观察数据
检查数据的最佳方法之一就是将其可视化。一种可视化方法是绘制散点图（scatter plot）。数据散点图将一个特征作为x 轴，另一个特征作为y 轴，将每一个数据点绘制为图上的一个点。

散点图矩阵无法同时显示所有特征之间的关系，所以这种可视化方法可能无法展示数据的某些有趣内容。

```python
# 利用iris_dataset.feature_names中的字符串对数据进行标记
iris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)
grr = pd.plotting.scatter_matrix(iris_dataframe, c=y_train, figsize=(15, 15), marker='o', hist_kwds={'bins': 20},cmap=mglearn.cm3)
plt.savefig("iris.png")
plt.show()

'''
scatter_matrix(frame, alpha=0.5, c,figsize=None, ax=None, diagonal='hist', marker='.', density_kwds=None,hist_kwds=None, range_padding=0.05, **kwds)
- frame pandas dataframe对象
- alpha 图像透明度，一般取(0,1]
- figsize 以英寸为单位的图像大小，一般以元组 (width, height) 形式设置
- ax 可选一般为none
- diagonal 必须且只能在{‘hist’, ‘kde’}中选择1个，’hist’表示直方图(Histogram plot),’kde’表示核密度估计(Kernel Density Estimation)；该参数是scatter_matrix函数的关键参数
- marker Matplotlib可用的标记类型，如’.’，’,’，’o’等
- density_kwds (other plotting keyword arguments，可选)，与kde相关的字典参数
- hist_kwds 与hist相关的字典参数
- range_padding (float, 可选)，图像在x轴、y轴原点附近的留白(padding)，该值越大，留白距离越大，图像远离坐标原点
- kwds 与scatter_matrix函数本身相关的字典参数
- c 颜色
'''
```
数据点的颜色与鸢尾花的品种相对应。为了绘制这张图，我们首先将NumPy 数组转换成pandas DataFrame。pandas 有一个绘制散点图矩阵的函数，叫作**scatter_matrix**【**矩阵的对角线是每个特征的直方图**】：

![iris](iris_matrix.png)

从图中可以看出，利用花瓣和花萼的测量数据基本可以将三个类别区分开。这说明机器学习模型很可能可以学会区分它们。

## 构建第一个模型：k近邻算法

这里我们用的是k 近邻分类器，这是一个很容易理解的算法。构建此模型只需要保存训练集即可。要对一个新的数据点做出预测，算法会在训练集中寻找与这个新数据点距离最近的数据点，然后将找到的数据点的标签赋值给这个新数据点。

k 近邻算法中k 的含义是，我们可以考虑训练集中与新数据点最近的任意k 个邻居（比如说，距离最近的3 个或5 个邻居），而不是只考虑最近的那一个。然后，我们可以用这些邻居中数量最多的类别做出预测。

```python
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train, y_train)
X_new = np.array([[5, 2.9, 1, 0.2]])
# print("X_new.shape{}".format(X_new.shape))
prediction = knn.predict(X_new)
print("prediction:{}".format(prediction))# [0]
print("predicted target name:{}".format(iris_dataset['target_names'][prediction]))# ['setosa']
```
## 评估模型
这里需要用到之前创建的测试集。这些数据没有用于构建模型，但我们知道测试集中每朵鸢尾花的实际品种。

因此，我们可以对测试数据中的每朵鸢尾花进行预测，并将预测结果与标签（已知的品种）进行对比。我们可以通过计算精度（accuracy）来衡量模型的优劣，精度就是品种预测正确的花所占的比例。
```python
y_pred = knn.predict((X_test))
print("test set predictions:{}".format(y_pred))
# print("test set score:{:.2f}".format(np.mean(y_pred == y_test)))
print("test set score:{:.2f}".format(knn.score(X_test, y_test)))
```